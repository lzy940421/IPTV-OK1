#!/usr/bin/env python3
import os
import asyncio
import configparser
from pathlib import Path
from typing import List, Set, Dict, Optional, Tuple, Callable
import re
import logging
import gc
import sys
from datetime import datetime
from collections import defaultdict
from core import (
    SourceFetcher,
    PlaylistParser,
    AutoCategoryMatcher,
    SpeedTester,
    ResultExporter,
    Channel
)
from core.progress import SmartProgress

# ==================== å·¥å…·å‡½æ•° ====================
def load_list_file(path: str) -> Set[str]:
    """åŠ è½½åå•æ–‡ä»¶ï¼ˆé»‘åå•/ç™½åå•ï¼‰"""
    file = Path(path)
    if not file.exists():
        return set()
    with open(file, 'r', encoding='utf-8') as f:
        return {line.strip().lower() for line in f if line.strip() and not line.startswith('#')}

def load_urls(path: str) -> List[str]:
    """åŠ è½½è®¢é˜…æºURLåˆ—è¡¨"""
    file = Path(path)
    if not file.exists():
        raise FileNotFoundError(f"è®¢é˜…æºæ–‡ä»¶ä¸å­˜åœ¨: {file}")
    with open(file, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f if line.strip()]

def is_blacklisted(channel: Channel, blacklist: Set[str]) -> bool:
    """æ£€æŸ¥é¢‘é“æ˜¯å¦åœ¨é»‘åå•ä¸­"""
    channel_name = channel.name.lower()
    channel_url = channel.url.lower()
    return any(
        entry in channel_name or entry in channel_url
        for entry in blacklist
        if entry.strip() and not entry.startswith('#')
    )

async def fetch_sources(fetcher: SourceFetcher, urls: List[str], logger: logging.Logger) -> List[str]:
    """è·å–è®¢é˜…æºå†…å®¹ï¼ˆå¸¦é‡è¯•ï¼‰"""
    contents = []
    for attempt in range(1, 3):
        try:
            progress = SmartProgress(len(urls), f"è·å–è®¢é˜…æº(å°è¯•{attempt}/2)")
            contents = await fetcher.fetch_all(urls, progress.update)
            progress.complete()
            break
        except Exception as e:
            if attempt == 2:
                raise
            logger.warning(f"ç¬¬{attempt}æ¬¡è·å–å¤±è´¥: {str(e)}")
            await asyncio.sleep(2 ** attempt)
    return [c for c in contents if c and c.strip()]

def parse_channels(parser: PlaylistParser, contents: List[str], logger: logging.Logger) -> List[Channel]:
    """è§£ææ‰€æœ‰é¢‘é“"""
    all_channels = []
    progress = SmartProgress(len(contents), "è§£æè¿›åº¦")
    
    for content in contents:
        try:
            channels = list(parser.parse(content))
            all_channels.extend(channels)
            
            if len(all_channels) % 5000 == 0:
                gc.collect()
                
            progress.update()
        except Exception as e:
            logger.error(f"è§£æå¼‚å¸¸: {str(e)}")
            continue
    
    progress.complete()
    return all_channels

def remove_duplicates(channels: List[Channel], logger: logging.Logger) -> List[Channel]:
    """å»é‡å¤„ç†"""
    progress = SmartProgress(len(channels), "å»é‡è¿›åº¦")
    unique_channels = {channel.url: channel for channel in channels}
    progress.update(len(channels))
    progress.complete()
    return list(unique_channels.values())

def filter_blacklist(channels: List[Channel], blacklist: Set[str], logger: logging.Logger) -> List[Channel]:
    """é»‘åå•è¿‡æ»¤"""
    if not blacklist:
        return channels
        
    progress = SmartProgress(len(channels), "è¿‡æ»¤è¿›åº¦")
    filtered = [c for c in channels if not is_blacklisted(c, blacklist)]
    progress.update(len(channels))
    progress.complete()
    return filtered

def classify_channels(matcher: AutoCategoryMatcher, channels: List[Channel], logger: logging.Logger) -> List[Channel]:
    """æ™ºèƒ½åˆ†ç±»"""
    progress = SmartProgress(len(channels), "åˆ†ç±»è¿›åº¦")
    
    # æ‰¹é‡åŒ¹é…åˆ†ç±»
    category_mapping = matcher.batch_match([c.name for c in channels])
    
    # åº”ç”¨åˆ†ç±»ç»“æœ
    processed = []
    for channel in channels:
        channel.category = category_mapping[channel.name]
        channel.name = matcher.normalize_channel_name(channel.name)
        processed.append(channel)
        progress.update()
    
    progress.complete()
    return processed

async def test_channels(tester: SpeedTester, channels: List[Channel], whitelist: Set[str], logger: logging.Logger) -> Set[str]:
    """æµ‹é€Ÿæµ‹è¯•"""
    if not channels:
        logger.warning("âš ï¸ æ— é¢‘é“éœ€è¦æµ‹é€Ÿ")
        return set()

    failed_urls = set()
    batch_size = min(5000, len(channels))
    progress = SmartProgress(len(channels), "æµ‹é€Ÿè¿›åº¦")
    
    for i in range(0, len(channels), batch_size):
        batch = channels[i:i+batch_size]
        await tester.test_channels(batch, progress.update, failed_urls, whitelist)
        gc.collect()
    
    progress.complete()
    return failed_urls

async def export_results(exporter: ResultExporter, channels: List[Channel], whitelist: Set[str], logger: logging.Logger) -> None:
    """ç»“æœå¯¼å‡º"""
    progress = SmartProgress(1, "å¯¼å‡ºè¿›åº¦")
    exporter.export(channels, whitelist, progress.update)  # åŒæ­¥è°ƒç”¨
    progress.complete()

# ==================== ä¸»æµç¨‹ ====================
def print_start_page(config: configparser.ConfigParser, logger: logging.Logger):
    """æ‰“å°ä¼˜åŒ–åçš„å¯åŠ¨é¡µé¢"""
    title = r"""
   ____   _   _   _       ___   _   _  __  __
  / ___| | \ | | | |     |_ _| | | | | \ \/ /
 | |     |  \| | | |      | |  | | | |  \  / 
 | |___  | |\  | | |___   | |  | |_| |  /  \ 
  \____| |_| \_| |_____| |___|  \___/  /_/\_\
    """
    
    # è·å–å…³é”®é…ç½®
    urls_path = config.get('PATHS', 'urls_path', fallback='config/urls.txt')
    templates_path = config.get('PATHS', 'templates_path', fallback='config/templates.txt')
    output_dir = config.get('MAIN', 'output_dir', fallback='outputs')
    uncategorized_path = config.get('PATHS', 'uncategorized_channels_path', fallback='config/uncategorized.txt')
    blacklist_path = config.get('BLACKLIST', 'blacklist_path', fallback='config/blacklist.txt')
    whitelist_path = config.get('WHITELIST', 'whitelist_path', fallback='config/whitelist.txt')
    failed_urls_path = config.get('PATHS', 'failed_urls_path', fallback='config/failed_urls.txt')
    log_file_path = config.get('LOGGING', 'log_file_path', fallback='outputs/debug.log')
    
    fetcher_timeout = config.getfloat('FETCHER', 'timeout', fallback=15)
    fetcher_concurrency = config.getint('FETCHER', 'concurrency', fallback=5)
    tester_timeout = config.getfloat('TESTER', 'timeout', fallback=10)
    tester_concurrency = config.getint('TESTER', 'concurrency', fallback=8)
    tester_logging = config.getboolean('TESTER', 'enable_logging', fallback=False)  # æ–°å¢æµ‹é€Ÿæ—¥å¿—å¼€å…³
    enable_history = config.getboolean('EXPORTER', 'enable_history', fallback=False)
    log_level = config.get('LOGGING', 'log_level', fallback='INFO').upper()
    
    # è·å–ç‰ˆæœ¬ä¿¡æ¯
    try:
        from core import __version__
        version = f"v{__version__}"
    except ImportError:
        version = "v1.0.0"
    
    # ä¼˜åŒ–åçš„å¯åŠ¨ä¿¡æ¯
    start_info = f"""
{title}
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  IPTVæ™ºèƒ½å¤„ç†ç³»ç»Ÿ {version.ljust(36)}  â•‘
â•‘  å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S').ljust(43)}â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘  å¤„ç†æµç¨‹:                                            â•‘
â•‘   1. è·å–è®¢é˜…æº â†’ 2. è§£æé¢‘é“ â†’ 3. å»é‡              â•‘
â•‘   4. é»‘åå•è¿‡æ»¤ â†’ 5. æ™ºèƒ½åˆ†ç±» â†’ 6. æµ‹é€Ÿæµ‹è¯•          â•‘
â•‘   7. ç»“æœå¯¼å‡º â†’ 8. å®Œæˆ!                             â•‘
â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢
â•‘  æ ¸å¿ƒé…ç½®æ¦‚è§ˆ:                                        â•‘
â•‘  â€¢ è®¢é˜…æºè·¯å¾„: {urls_path.ljust(46)}â•‘
â•‘  â€¢ åˆ†ç±»æ¨¡æ¿: {templates_path.ljust(48)}â•‘
â•‘  â€¢ æœªåˆ†ç±»é¢‘é“è·¯å¾„: {uncategorized_path.ljust(41)}â•‘
â•‘  â€¢ é»‘åå•è·¯å¾„: {blacklist_path.ljust(45)}â•‘
â•‘  â€¢ ç™½åå•è·¯å¾„: {whitelist_path.ljust(45)}â•‘
â•‘  â€¢ å¤±è´¥URLè·¯å¾„: {failed_urls_path.ljust(43)}â•‘
â•‘  â€¢ æ—¥å¿—æ–‡ä»¶è·¯å¾„: {log_file_path.ljust(42)}â•‘
â•‘  â€¢ è¾“å‡ºç›®å½•: {output_dir.ljust(46)}â•‘
â•‘  â€¢ æŠ“å–å¹¶å‘æ•°: {str(fetcher_concurrency).ljust(3)} è¶…æ—¶: {str(fetcher_timeout).ljust(4)}ç§’          â•‘
â•‘  â€¢ æµ‹é€Ÿå¹¶å‘æ•°: {str(tester_concurrency).ljust(3)} è¶…æ—¶: {str(tester_timeout).ljust(4)}ç§’          â•‘
â•‘  â€¢ æµ‹é€Ÿæ—¥å¿—: {'å¯ç”¨' if tester_logging else 'ç¦ç”¨'.ljust(45)}â•‘
â•‘  â€¢ å†å²è®°å½•: {'å¯ç”¨' if enable_history else 'ç¦ç”¨'.ljust(45)}â•‘
â•‘  â€¢ æ—¥å¿—çº§åˆ«: {log_level.ljust(46)}â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    logger.info(start_info)

def setup_logging(config: configparser.ConfigParser) -> logging.Logger:
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    logger = logging.getLogger()
    logger.setLevel(config.get('LOGGING', 'log_level', fallback='INFO').upper())

    for handler in logger.handlers[:]:
        logger.removeHandler(handler)

    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    ))
    logger.addHandler(console_handler)

    if config.getboolean('LOGGING', 'log_to_file', fallback=False):
        log_file = Path(config.get('LOGGING', 'log_file_path', fallback='outputs/debug.log'))
        log_file.parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
        file_handler.setFormatter(logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        ))
        logger.addHandler(file_handler)
    
    return logger

async def main():
    """ä¸»å·¥ä½œæµç¨‹ï¼ˆå®Œæ•´ä¿®å¤ç‰ˆï¼‰"""
    try:
        # ==================== åˆå§‹åŒ–é˜¶æ®µ ====================
        print("="*60)
        config = configparser.ConfigParser()
        config.read('config/config.ini', encoding='utf-8')
        logger = setup_logging(config)
        logger.info("âœ… é…ç½®åŠ è½½å®Œæˆ")
        print_start_page(config, logger)

        # ==================== æ•°æ®å‡†å¤‡é˜¶æ®µ ====================
        logger.info("\nğŸ”¹ğŸ”¹ é˜¶æ®µ1/7ï¼šæ•°æ®å‡†å¤‡")
        blacklist = load_list_file(config.get('BLACKLIST', 'blacklist_path', fallback='config/blacklist.txt'))
        whitelist = load_list_file(config.get('WHITELIST', 'whitelist_path', fallback='config/whitelist.txt'))
        urls = load_urls(config.get('PATHS', 'urls_path', fallback='config/urls.txt'))
        logger.info(f"â€¢ åŠ è½½é»‘åå•: {len(blacklist)}æ¡")
        logger.info(f"â€¢ åŠ è½½ç™½åå•: {len(whitelist)}æ¡")
        logger.info(f"â€¢ åŠ è½½è®¢é˜…æº: {len(urls)}ä¸ª")

        # ==================== è®¢é˜…æºè·å–é˜¶æ®µ ====================
        logger.info("\nğŸ”¹ğŸ”¹ é˜¶æ®µ2/7ï¼šè·å–è®¢é˜…æº")
        fetcher = SourceFetcher(
            timeout=config.getfloat('FETCHER', 'timeout', fallback=15),
            concurrency=config.getint('FETCHER', 'concurrency', fallback=5),
            config=config
        )
        contents = await fetch_sources(fetcher, urls, logger)
        logger.info(f"âœ… è·å–å®Œæˆ | æˆåŠŸ: {len(contents)}/{len(urls)}")

        # ==================== é¢‘é“è§£æé˜¶æ®µ ====================
        logger.info("\nğŸ”¹ğŸ”¹ é˜¶æ®µ3/7ï¼šè§£æé¢‘é“")
        parser = PlaylistParser(config)
        all_channels = parse_channels(parser, contents, logger)
        unique_sources = len({c.url for c in all_channels})
        logger.info(f"âœ… è§£æå®Œæˆ | æ€»é¢‘é“: {len(all_channels)} | å”¯ä¸€æº: {unique_sources}")

        # ==================== æ•°æ®å¤„ç†é˜¶æ®µ ====================
        logger.info("\nğŸ”¹ğŸ”¹ é˜¶æ®µ4/7ï¼šæ•°æ®å¤„ç†")
        unique_channels = remove_duplicates(all_channels, logger)
        filtered_channels = filter_blacklist(unique_channels, blacklist, logger)
        logger.info(f"âœ” å¤„ç†å®Œæˆ | å»é‡å: {len(unique_channels)} | è¿‡æ»¤å: {len(filtered_channels)}")

        # ==================== æ™ºèƒ½åˆ†ç±»é˜¶æ®µ ====================
        logger.info("\nğŸ”¹ğŸ”¹ é˜¶æ®µ5/7ï¼šæ™ºèƒ½åˆ†ç±»")
        matcher = AutoCategoryMatcher(
            config.get('PATHS', 'templates_path', fallback='config/templates.txt'),
            config
        )
        processed_channels = classify_channels(matcher, filtered_channels, logger)
        classified = sum(1 for c in processed_channels if c.category != "æœªåˆ†ç±»")
        logger.info(f"âœ… åˆ†ç±»å®Œæˆ | å·²åˆ†ç±»: {classified} | æœªåˆ†ç±»: {len(processed_channels)-classified}")

        # ==================== æµ‹é€Ÿæµ‹è¯•é˜¶æ®µ ====================
        logger.info("\nğŸ”¹ğŸ”¹ é˜¶æ®µ6/7ï¼šæµ‹é€Ÿæµ‹è¯•")
        tester = SpeedTester(
            timeout=config.getfloat('TESTER', 'timeout', fallback=10),
            concurrency=config.getint('TESTER', 'concurrency', fallback=8),
            max_attempts=config.getint('TESTER', 'max_attempts', fallback=2),
            min_download_speed=config.getfloat('TESTER', 'min_download_speed', fallback=0.1),
            enable_logging=config.getboolean('TESTER', 'enable_logging', fallback=False),  # å…³é”®ä¿®å¤ç‚¹
            config=config
        )
        sorted_channels = matcher.sort_channels_by_template(processed_channels, whitelist)
        failed_urls = await test_channels(tester, sorted_channels, whitelist, logger)
        online_count = sum(1 for c in sorted_channels if c.status == 'online')
        logger.info(f"âœ… æµ‹é€Ÿå®Œæˆ | åœ¨çº¿: {online_count}/{len(sorted_channels)} | å¤±è´¥: {len(failed_urls)}")

        # ==================== ç»“æœå¯¼å‡ºé˜¶æ®µ ====================
        logger.info("\nğŸ”¹ğŸ”¹ é˜¶æ®µ7/7ï¼šç»“æœå¯¼å‡º")
        exporter = ResultExporter(
            output_dir=config.get('MAIN', 'output_dir', fallback='outputs'),
            template_path=config.get('PATHS', 'templates_path'),
            config=config,
            matcher=matcher
        )
        await export_results(exporter, sorted_channels, whitelist, logger)

        # ==================== æœ€ç»ˆç»Ÿè®¡ ====================
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š æœ€ç»ˆç»Ÿè®¡")
        logger.info(f"â€¢ æ€»å¤„ç†é¢‘é“: {len(sorted_channels)}")
        logger.info(f"â€¢ åœ¨çº¿é¢‘é“: {online_count} (æˆåŠŸç‡: {online_count/len(sorted_channels)*100:.1f}%)")
        logger.info(f"â€¢ æœªåˆ†ç±»é¢‘é“: {len(processed_channels)-classified}")
        logger.info("="*60 + "\nğŸ‰ ä»»åŠ¡å®Œæˆï¼")

    except KeyboardInterrupt:
        logger.error("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(0)
    except Exception as e:
        logger.error("\n" + "â€¼ï¸"*20)
        logger.error(f"è‡´å‘½é”™è¯¯: {str(e)}", exc_info=True)
        logger.error("â€¼ï¸"*20)
        sys.exit(1)

if __name__ == "__main__":
    # Windowsç³»ç»Ÿè®¾ç½®äº‹ä»¶å¾ªç¯ç­–ç•¥
    if os.name == 'nt':
        from asyncio import WindowsSelectorEventLoopPolicy
        asyncio.set_event_loop_policy(WindowsSelectorEventLoopPolicy())

    # åˆå§‹åŒ–æ—¥å¿—ï¼ˆä¸´æ—¶ç”¨äºé…ç½®åŠ è½½ï¼‰
    logging.basicConfig(level=logging.INFO)
    temp_logger = logging.getLogger()

    try:
        # åŠ è½½é…ç½®
        config = configparser.ConfigParser()
        config.read('config/config.ini', encoding='utf-8')
        
        # é‡æ–°é…ç½®æ—¥å¿—
        logger = setup_logging(config)
        
        # è¿è¡Œä¸»ç¨‹åº
        asyncio.run(main())
    except Exception as e:
        temp_logger.error(f"å¯åŠ¨å¤±è´¥: {str(e)}", exc_info=True)
        sys.exit(1)